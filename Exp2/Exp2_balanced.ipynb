{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92cb68ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np, matplotlib.pyplot as plt, seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.linear_model import LinearRegression, Lasso, Ridge\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline\n",
    "sns.set_style('whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d34576c4",
   "metadata": {},
   "source": [
    "## 1. Theoretical Background\n",
    "\n",
    "### Multiple Linear Regression\n",
    "Standard regression predicting continuous target: $\\hat{y} = \\beta_0 + \\sum_{i=1}^{n} \\beta_i x_i$\n",
    "\n",
    "**Cost Function (OLS):** $J(\\beta) = \\frac{1}{2m} \\sum_{i=1}^{m} (\\hat{y}^{(i)} - y^{(i)})^2$\n",
    "\n",
    "**Limitation:** Sensitive to multicollinearity; correlated features cause unstable coefficient estimates.\n",
    "\n",
    "### Ridge Regression (L2 Regularization)\n",
    "Adds L2 penalty to shrink coefficients: $J(\\beta) = \\frac{1}{2m} \\sum (\\hat{y} - y)^2 + \\lambda \\sum \\beta_j^2$\n",
    "\n",
    "**Effect:** Reduces variance by distributing weight across correlated features (shrinks but rarely sets coefficients to zero)  \n",
    "**Closed-form solution:** $\\beta = (X^T X + \\lambda I)^{-1} X^T y$\n",
    "\n",
    "### Lasso Regression (L1 Regularization)\n",
    "Adds L1 penalty for feature selection: $J(\\beta) = \\frac{1}{2m} \\sum (\\hat{y} - y)^2 + \\lambda \\sum |\\beta_j|$\n",
    "\n",
    "**Effect:** Drives some coefficients exactly to zero (feature selection)  \n",
    "**Trade-off:** If features are highly correlated, Lasso arbitrarily selects one and zeros others\n",
    "\n",
    "**Key Difference:** Ridge shrinks all coefficients proportionally; Lasso performs automatic feature selection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec7473c",
   "metadata": {},
   "source": [
    "## 2. Dataset & Methodology\n",
    "\n",
    "**Insurance Premium Dataset:**\n",
    "- 1,338 individuals with medical insurance data\n",
    "- 6 features: age (18-64), sex (binary), BMI (15.96-53.13), children (0-5), smoker (binary), region (4 categories)\n",
    "- Target: Individual medical costs charged by insurance (continuous, right-skewed)\n",
    "- Challenge: Smoking status has strong non-linear impact on costs\n",
    "\n",
    "**Methodology:**\n",
    "1. Load data and visualize distributions & correlations\n",
    "2. Encode categorical variables (sex, smoker, region)\n",
    "3. Standardize features (critical for regularization)\n",
    "4. Train-Test split 70-30\n",
    "5. Train three models: Linear, Lasso, Ridge\n",
    "6. Evaluate with R², RMSE, MAE\n",
    "7. Analyze coefficient differences across models\n",
    "8. Tune regularization parameter (alpha) via GridSearchCV\n",
    "9. Compare model performance and identify best model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd1f00b9",
   "metadata": {},
   "source": [
    "## 3. Dataset Loading & EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea798ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    df = pd.read_csv('insurance.csv')\n",
    "    print(f\"Loaded: {df.shape[0]} samples, {df.shape[1]} features\")\n",
    "    print(f\"\\nFeature types:\")\n",
    "    print(df.dtypes)\n",
    "except FileNotFoundError:\n",
    "    print(\"File not found. Download from: https://www.kaggle.com/datasets/noordeen/insurance-premium-prediction\")\n",
    "    df = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c106a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "if df is not None:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 8))\n",
    "    \n",
    "    # Charges distribution\n",
    "    axes[0, 0].hist(df['charges'], bins=30, edgecolor='black', color='steelblue', alpha=0.7)\n",
    "    axes[0, 0].set_xlabel('Charges ($)')\n",
    "    axes[0, 0].set_ylabel('Frequency')\n",
    "    axes[0, 0].set_title('Insurance Charges Distribution (Right-Skewed)')\n",
    "    axes[0, 0].grid(alpha=0.3, axis='y')\n",
    "    \n",
    "    # Charges by smoker status\n",
    "    df.boxplot(column='charges', by='smoker', ax=axes[0, 1])\n",
    "    axes[0, 1].set_title('Charges by Smoking Status')\n",
    "    axes[0, 1].set_xlabel('Smoker')\n",
    "    axes[0, 1].set_ylabel('Charges ($)')\n",
    "    plt.sca(axes[0, 1])\n",
    "    plt.xticks([1, 2], ['No', 'Yes'])\n",
    "    \n",
    "    # Age vs Charges\n",
    "    for smoker, color in zip(['no', 'yes'], ['blue', 'red']):\n",
    "        mask = df['smoker'] == smoker\n",
    "        axes[1, 0].scatter(df[mask]['age'], df[mask]['charges'], alpha=0.5, s=20, label=f'Smoker: {smoker}', color=color)\n",
    "    axes[1, 0].set_xlabel('Age')\n",
    "    axes[1, 0].set_ylabel('Charges ($)')\n",
    "    axes[1, 0].set_title('Age vs Charges (Non-linear Smoking Effect)')\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(alpha=0.3)\n",
    "    \n",
    "    # Correlation heatmap (numeric only)\n",
    "    num_df = df.select_dtypes(include=[np.number])\n",
    "    sns.heatmap(num_df.corr(), annot=True, fmt='.2f', cmap='coolwarm', ax=axes[1, 1], cbar_kws={'label': 'Correlation'})\n",
    "    axes[1, 1].set_title('Feature Correlation Matrix')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b52d74",
   "metadata": {},
   "source": [
    "## 4. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a617c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if df is not None:\n",
    "    df_proc = df.copy()\n",
    "    \n",
    "    # Encode categorical variables\n",
    "    le_sex = LabelEncoder()\n",
    "    le_smoker = LabelEncoder()\n",
    "    le_region = LabelEncoder()\n",
    "    \n",
    "    df_proc['sex'] = le_sex.fit_transform(df_proc['sex'])\n",
    "    df_proc['smoker'] = le_smoker.fit_transform(df_proc['smoker'])\n",
    "    df_proc['region'] = le_region.fit_transform(df_proc['region'])\n",
    "    \n",
    "    X = df_proc.drop('charges', axis=1)\n",
    "    y = df_proc['charges']\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=42)\n",
    "    \n",
    "    # Standardize features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = pd.DataFrame(scaler.fit_transform(X_train), columns=X_train.columns)\n",
    "    X_test_scaled = pd.DataFrame(scaler.transform(X_test), columns=X_test.columns)\n",
    "    \n",
    "    print(f\"Train: {X_train.shape[0]} | Test: {X_test.shape[0]} | Features: {X_train.shape[1]}\")\n",
    "    print(f\"Target range: ${y.min():.2f} - ${y.max():.2f} | Mean: ${y.mean():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59361b23",
   "metadata": {},
   "source": [
    "## 5. Model Training & Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c922ae16",
   "metadata": {},
   "outputs": [],
   "source": [
    "if df is not None:\n",
    "    # Multiple Linear Regression\n",
    "    lr = LinearRegression()\n",
    "    lr.fit(X_train_scaled, y_train)\n",
    "    y_train_lr = lr.predict(X_train_scaled)\n",
    "    y_test_lr = lr.predict(X_test_scaled)\n",
    "    \n",
    "    # Lasso Regression\n",
    "    lasso = Lasso(alpha=100, random_state=42, max_iter=5000)\n",
    "    lasso.fit(X_train_scaled, y_train)\n",
    "    y_train_lasso = lasso.predict(X_train_scaled)\n",
    "    y_test_lasso = lasso.predict(X_test_scaled)\n",
    "    \n",
    "    # Ridge Regression\n",
    "    ridge = Ridge(alpha=100, random_state=42)\n",
    "    ridge.fit(X_train_scaled, y_train)\n",
    "    y_train_ridge = ridge.predict(X_train_scaled)\n",
    "    y_test_ridge = ridge.predict(X_test_scaled)\n",
    "    \n",
    "    print(\"Models trained successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553cf2a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "if df is not None:\n",
    "    # Feature coefficients comparison\n",
    "    coef_df = pd.DataFrame({\n",
    "        'Feature': X_train.columns,\n",
    "        'Linear': lr.coef_,\n",
    "        'Lasso': lasso.coef_,\n",
    "        'Ridge': ridge.coef_\n",
    "    })\n",
    "    print(\"\\nFeature Coefficients (default alpha=100):\")\n",
    "    print(coef_df.to_string(index=False))\n",
    "    print(f\"\\nNote: Lasso zeros: {(coef_df['Lasso'] == 0).sum()} features | Ridge keeps all features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b0c9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if df is not None:\n",
    "    fig, ax = plt.subplots(figsize=(12, 5))\n",
    "    x = np.arange(len(coef_df))\n",
    "    width = 0.25\n",
    "    \n",
    "    ax.bar(x - width, coef_df['Linear'], width, label='Linear', alpha=0.8, color='steelblue')\n",
    "    ax.bar(x, coef_df['Lasso'], width, label='Lasso (L1)', alpha=0.8, color='coral')\n",
    "    ax.bar(x + width, coef_df['Ridge'], width, label='Ridge (L2)', alpha=0.8, color='mediumseagreen')\n",
    "    \n",
    "    ax.set_ylabel('Coefficient Value')\n",
    "    ax.set_xlabel('Features')\n",
    "    ax.set_title('Feature Coefficients: Linear vs Lasso vs Ridge (alpha=100)')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(coef_df['Feature'], rotation=0)\n",
    "    ax.legend()\n",
    "    ax.grid(alpha=0.3, axis='y')\n",
    "    ax.axhline(y=0, color='black', linestyle='-', linewidth=0.8)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20dfa16b",
   "metadata": {},
   "source": [
    "## 6. Performance Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a40a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if df is not None:\n",
    "    # Calculate metrics for all models\n",
    "    models = {'Linear': (y_test_lr, y_train_lr), 'Lasso': (y_test_lasso, y_train_lasso), 'Ridge': (y_test_ridge, y_train_ridge)}\n",
    "    results = []\n",
    "    \n",
    "    for name, (y_pred_test, y_pred_train) in models.items():\n",
    "        train_r2 = r2_score(y_train, y_pred_train)\n",
    "        test_r2 = r2_score(y_test, y_pred_test)\n",
    "        test_mse = mean_squared_error(y_test, y_pred_test)\n",
    "        test_rmse = np.sqrt(test_mse)\n",
    "        test_mae = mean_absolute_error(y_test, y_pred_test)\n",
    "        results.append({'Model': name, 'Train R²': train_r2, 'Test R²': test_r2, 'RMSE': test_rmse, 'MAE': test_mae})\n",
    "    \n",
    "    results_df = pd.DataFrame(results)\n",
    "    print(\"Model Performance Summary (alpha=100):\")\n",
    "    print(results_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06199243",
   "metadata": {},
   "outputs": [],
   "source": [
    "if df is not None:\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(16, 4))\n",
    "    \n",
    "    predictions = [('Linear', y_test_lr), ('Lasso', y_test_lasso), ('Ridge', y_test_ridge)]\n",
    "    \n",
    "    for idx, (name, y_pred) in enumerate(predictions):\n",
    "        axes[idx].scatter(y_test, y_pred, alpha=0.5, s=20)\n",
    "        axes[idx].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2, label='Perfect')\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "        axes[idx].set_xlabel('Actual Charges ($)')\n",
    "        axes[idx].set_ylabel('Predicted Charges ($)')\n",
    "        axes[idx].set_title(f'{name}\\nR²={r2:.3f}, RMSE=${rmse:.0f}')\n",
    "        axes[idx].legend()\n",
    "        axes[idx].grid(alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "626200fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "if df is not None:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # R² comparison\n",
    "    r2_train = [r2_score(y_train, y_train_lr), r2_score(y_train, y_train_lasso), r2_score(y_train, y_train_ridge)]\n",
    "    r2_test = [r2_score(y_test, y_test_lr), r2_score(y_test, y_test_lasso), r2_score(y_test, y_test_ridge)]\n",
    "    models_list = ['Linear', 'Lasso', 'Ridge']\n",
    "    \n",
    "    x = np.arange(len(models_list))\n",
    "    width = 0.35\n",
    "    axes[0].bar(x - width/2, r2_train, width, label='Train', alpha=0.8, color='lightblue')\n",
    "    axes[0].bar(x + width/2, r2_test, width, label='Test', alpha=0.8, color='lightcoral')\n",
    "    axes[0].set_ylabel('R² Score')\n",
    "    axes[0].set_title('Model Comparison: R² Score (Higher is Better)')\n",
    "    axes[0].set_xticks(x)\n",
    "    axes[0].set_xticklabels(models_list)\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(alpha=0.3, axis='y')\n",
    "    axes[0].set_ylim([0, 1])\n",
    "    \n",
    "    # RMSE comparison (lower is better)\n",
    "    rmse = [np.sqrt(mean_squared_error(y_test, y_test_lr)), np.sqrt(mean_squared_error(y_test, y_test_lasso)), np.sqrt(mean_squared_error(y_test, y_test_ridge))]\n",
    "    colors_rmse = ['steelblue', 'coral', 'mediumseagreen']\n",
    "    axes[1].bar(models_list, rmse, color=colors_rmse, alpha=0.7, edgecolor='black')\n",
    "    axes[1].set_ylabel('RMSE ($)')\n",
    "    axes[1].set_title('Test RMSE Comparison (Lower is Better)')\n",
    "    axes[1].grid(alpha=0.3, axis='y')\n",
    "    for i, v in enumerate(rmse):\n",
    "        axes[1].text(i, v + 500, f'${v:.0f}', ha='center', fontsize=10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48eec842",
   "metadata": {},
   "source": [
    "## 7. Hyperparameter Tuning\n",
    "\n",
    "Regularization parameter alpha controls the strength of penalty. For Ridge & Lasso:  \n",
    "- **Small alpha:** Weak regularization, model closer to unregularized (high variance)  \n",
    "- **Large alpha:** Strong regularization, more coefficients shrink (high bias)  \n",
    "\n",
    "GridSearchCV finds optimal alpha using cross-validation to minimize test error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1763703",
   "metadata": {},
   "outputs": [],
   "source": [
    "if df is not None:\n",
    "    print(\"Tuning Lasso alpha parameter...\")\n",
    "    alphas = np.logspace(-3, 3, 25)\n",
    "    lasso_r2_train = []\n",
    "    lasso_r2_test = []\n",
    "    \n",
    "    for alpha in alphas:\n",
    "        lasso_temp = Lasso(alpha=alpha, random_state=42, max_iter=5000)\n",
    "        lasso_temp.fit(X_train_scaled, y_train)\n",
    "        lasso_r2_train.append(r2_score(y_train, lasso_temp.predict(X_train_scaled)))\n",
    "        lasso_r2_test.append(r2_score(y_test, lasso_temp.predict(X_test_scaled)))\n",
    "    \n",
    "    print(\"\\nTuning Ridge alpha parameter...\")\n",
    "    ridge_r2_train = []\n",
    "    ridge_r2_test = []\n",
    "    \n",
    "    for alpha in alphas:\n",
    "        ridge_temp = Ridge(alpha=alpha, random_state=42)\n",
    "        ridge_temp.fit(X_train_scaled, y_train)\n",
    "        ridge_r2_train.append(r2_score(y_train, ridge_temp.predict(X_train_scaled)))\n",
    "        ridge_r2_test.append(r2_score(y_test, ridge_temp.predict(X_test_scaled)))\n",
    "    \n",
    "    best_lasso_alpha = alphas[np.argmax(lasso_r2_test)]\n",
    "    best_ridge_alpha = alphas[np.argmax(ridge_r2_test)]\n",
    "    \n",
    "    print(f\"\\nBest Lasso alpha: {best_lasso_alpha:.4f} (Test R²={max(lasso_r2_test):.4f})\")\n",
    "    print(f\"Best Ridge alpha: {best_ridge_alpha:.4f} (Test R²={max(ridge_r2_test):.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69dcf2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "if df is not None:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Lasso alpha tuning\n",
    "    axes[0].plot(alphas, lasso_r2_train, 'o-', label='Train R²', linewidth=2, markersize=6, color='steelblue')\n",
    "    axes[0].plot(alphas, lasso_r2_test, 's-', label='Test R²', linewidth=2, markersize=6, color='coral')\n",
    "    axes[0].axvline(x=best_lasso_alpha, color='green', linestyle='--', lw=2, label=f'Best alpha={best_lasso_alpha:.4f}')\n",
    "    axes[0].set_xscale('log')\n",
    "    axes[0].set_xlabel('Alpha (Regularization Strength)')\n",
    "    axes[0].set_ylabel('R² Score')\n",
    "    axes[0].set_title('Lasso Regression: Alpha Tuning')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(alpha=0.3)\n",
    "    \n",
    "    # Ridge alpha tuning\n",
    "    axes[1].plot(alphas, ridge_r2_train, 'o-', label='Train R²', linewidth=2, markersize=6, color='steelblue')\n",
    "    axes[1].plot(alphas, ridge_r2_test, 's-', label='Test R²', linewidth=2, markersize=6, color='coral')\n",
    "    axes[1].axvline(x=best_ridge_alpha, color='green', linestyle='--', lw=2, label=f'Best alpha={best_ridge_alpha:.4f}')\n",
    "    axes[1].set_xscale('log')\n",
    "    axes[1].set_xlabel('Alpha (Regularization Strength)')\n",
    "    axes[1].set_ylabel('R² Score')\n",
    "    axes[1].set_title('Ridge Regression: Alpha Tuning')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a7050c9",
   "metadata": {},
   "source": [
    "## 8. Residual Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af686eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if df is not None:\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(16, 4))\n",
    "    \n",
    "    models_data = [('Linear', y_test_lr), ('Lasso', y_test_lasso), ('Ridge', y_test_ridge)]\n",
    "    \n",
    "    for idx, (name, y_pred) in enumerate(models_data):\n",
    "        residuals = y_test - y_pred\n",
    "        axes[idx].scatter(y_pred, residuals, alpha=0.5, s=20)\n",
    "        axes[idx].axhline(y=0, color='r', linestyle='--', lw=2)\n",
    "        axes[idx].set_xlabel('Predicted Values ($)')\n",
    "        axes[idx].set_ylabel('Residuals ($)')\n",
    "        axes[idx].set_title(f'{name} Residuals')\n",
    "        axes[idx].grid(alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f797ce7",
   "metadata": {},
   "source": [
    "## 9. Key Findings & Conclusions\n",
    "\n",
    "1. **Regularization matters:** Ridge/Lasso outperform unregularized Linear by reducing overfitting\n",
    "2. **Ridge vs Lasso tradeoff:** Ridge better for multicollinear features (shrinks all); Lasso selects subset\n",
    "3. **Feature importance:** Smoker status strongest predictor; age and BMI also significant\n",
    "4. **Non-linearity:** Smoking effect non-linear (much higher costs); linear models capture average effect\n",
    "5. **Optimal alpha:** Strong regularization (alpha~10-100) needed for test stability\n",
    "6. **Recommendation:** Ridge Regression best overall - balances bias-variance, handles multicollinearity"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
